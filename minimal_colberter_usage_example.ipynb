{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng63tDwZSSm5"
      },
      "source": [
        "# Using Our ColBERTer Checkpoints\n",
        "\n",
        "This notebook gives you a minimal usage example of downloading our ColBERTer checkpoint to encode passages and queries & to create a score of their relevance. \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Let's get started by installing the awesome *transformers* library from HuggingFace:\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "r2WyNOE2R2rW"
      },
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqkWDa_jWu7c"
      },
      "source": [
        "The next step is to download our checkpoint and initialize the tokenizer and models:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTYEtziISSDl"
      },
      "outputs": [],
      "source": [
        "from colberter import ColBERTer\n",
        "from bow2_tokenizer import BOW2Tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "#\n",
        "# init the model & tokenizer (using the distilbert tokenizer)\n",
        "#\n",
        "tokenizer = BOW2Tokenizer(AutoTokenizer.from_pretrained(\"distilbert-base-uncased\"))\n",
        "\n",
        "# we have 2 models available: a 32 dim model and a 1 dim model, please select either one of them\n",
        "model = ColBERTer.from_pretrained(\"sebastian-hofstaetter/colberter-128-32-msmarco\")\n",
        "#model = ColBERTer.from_pretrained(\"sebastian-hofstaetter/colberter-128-1-msmarco\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOGT8YQQX1Ot"
      },
      "source": [
        "# Pairwise scoring (for training & re-ranking)\n",
        "\n",
        "Now we are ready to use the model to encode two sample passage and query pairs (this would be the re-ranking mode, where we have a candidate list):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rzt9Ix9UYMLy",
        "outputId": "529e338e-b4e7-4251-cf9b-4363ac8a3ed8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---\n",
            "Score passage 1 <-> query:  39.04731750488281\n",
            "Score passage 2 <-> query:  32.5419807434082\n"
          ]
        }
      ],
      "source": [
        "# our relevant example\n",
        "passage1_input = tokenizer.tokenize(\"We are very happy to show you the ü§ó Transformers library for pre-trained language models. We are helping the community work together towards the goal of advancing NLP üî•.\")\n",
        "# a non-relevant example\n",
        "passage2_input = tokenizer.tokenize(\"Hmm I don't like this new movie about transformers that i got from my local library. Those transformers are robots?\")\n",
        "\n",
        "# the user query -> which should give us a better score for the first passage\n",
        "query_input = tokenizer.tokenize(\"what is the transformers library\")\n",
        "\n",
        "#print(\"Passage 1 Tokenized:\",passage1_input)\n",
        "#print(\"Passage 2 Tokenized:\",passage2_input)\n",
        "#print(\"Query Tokenized:\",query_input)\n",
        "\n",
        "# note how we call the ColBERTer model for pairs, can be changed to: forward_representation and forward_aggregation\n",
        "# set fp16=False if run on a CPU\n",
        "score_for_p1 = model.forward(query_input,passage1_input,use_fp16=False)[0].squeeze(0)\n",
        "score_for_p2 = model.forward(query_input,passage2_input,use_fp16=False)[0].squeeze(0)\n",
        "\n",
        "print(\"---\")\n",
        "print(\"Score passage 1 <-> query: \",float(score_for_p1))\n",
        "print(\"Score passage 2 <-> query: \",float(score_for_p2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOGT8YQQX1Ot"
      },
      "source": [
        "# Separate Encoding & Scoring (For Pre-Computation & Indexing)\n",
        "\n",
        "For indexing or pre-compute mode you need to call forward_representation and forward_aggregation independently:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rzt9Ix9UYMLy",
        "outputId": "529e338e-b4e7-4251-cf9b-4363ac8a3ed8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---\n",
            "Score passage 1 <-> query:  39.04731750488281 (CLS score: 16.83791732788086 )\n",
            "Score passage 2 <-> query:  32.5419807434082 (CLS score: 15.022980690002441 )\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# we re-use the tokenized input from the previous cell, and independently encode all 3 sequences\n",
        "p1_encoded_cls, p1_encoded_bow, p1_encoded_bow_mask = model.forward_representation(passage1_input,sequence_type=\"doc_encode\")\n",
        "p2_encoded_cls, p2_encoded_bow, p2_encoded_bow_mask = model.forward_representation(passage2_input,sequence_type=\"doc_encode\")\n",
        "\n",
        "#\n",
        "# let's assume at this point the p1_encoded_cls and p2_encoded_cls get index in an ANN, \n",
        "#   the bow encodings get saved by id -> workflow #2 (in the paper)\n",
        "#\n",
        "\n",
        "# now we get a query in from the user and encode it\n",
        "q_encoded_cls, q_encoded_bow, q_encoded_bow_mask = model.forward_representation(query_input,sequence_type=\"query_encode\")\n",
        "\n",
        "# this is done by the ANN index\n",
        "cls_score_p1 = p1_encoded_cls @ q_encoded_cls.T\n",
        "cls_score_p2 = p2_encoded_cls @ q_encoded_cls.T\n",
        "\n",
        "# now we assume that the two passages have been returned by the ANN index\n",
        "exact_scoring_mask_p1=None\n",
        "exact_scoring_mask_p2=None\n",
        "\n",
        "if model.compress_to_exact_mini_mode:\n",
        "    print(\"Using exact matching\")\n",
        "    exact_scoring_mask_p1 = query_input[\"unique_words\"].unsqueeze(-1) == passage1_input[\"unique_words\"].unsqueeze(1)\n",
        "    exact_scoring_mask_p2 = query_input[\"unique_words\"].unsqueeze(-1) == passage2_input[\"unique_words\"].unsqueeze(1)\n",
        "\n",
        "score_for_p1 = model.forward_aggregation(cls_score_p1,\n",
        "                                         q_encoded_bow,q_encoded_bow_mask,\n",
        "                                         p1_encoded_bow,p1_encoded_bow_mask,\n",
        "                                         exact_scoring_mask=exact_scoring_mask_p1)\n",
        "\n",
        "score_for_p2 = model.forward_aggregation(cls_score_p2,\n",
        "                                         q_encoded_bow,q_encoded_bow_mask,\n",
        "                                         p2_encoded_bow,p2_encoded_bow_mask,\n",
        "                                         exact_scoring_mask=exact_scoring_mask_p2)\n",
        "\n",
        "print(\"---\")\n",
        "print(\"Score passage 1 <-> query: \",float(score_for_p1),\"(CLS score:\",float(cls_score_p1*torch.sigmoid(model.score_merger)),\")\")\n",
        "print(\"Score passage 2 <-> query: \",float(score_for_p2),\"(CLS score:\",float(cls_score_p2*torch.sigmoid(model.score_merger)),\")\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1bY5qB9b-AI"
      },
      "source": [
        "As we see the model gives the first passage a higher score than the second - these scores would now be used to generate a list (if we run this comparison on all passages in our collection or candidate set). \n",
        "\n",
        "- If you want to look at more complex usages and training code we have a library for that: https://github.com/sebastian-hofstaetter/matchmaker üëè\n",
        "\n",
        "- If you use our model checkpoint please cite our work as:\n",
        "\n",
        "    ```\n",
        "@article{Hofstaetter2022_colberter,\n",
        " author = {Sebastian Hofst{\\\"a}tter and Omar Khattab and Sophia Althammer and Mete Sertkan and Allan Hanbury},\n",
        " title = {Introducing Neural Bag of Whole-Words with ColBERTer: Contextualized Late Interactions using Enhanced Reduction},\n",
        " publisher = {arXiv},\n",
        " url = {https://arxiv.org/abs/2203.13088},\n",
        " doi = {10.48550/ARXIV.2203.13088},\n",
        " year = {2022},\n",
        "}\n",
        "    ```\n",
        "\n",
        "Thank You üòä If you have any questions feel free to reach out to Sebastian via mail. \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "minimal_colbert_usage_example.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "d257a221e7c89d88c7f66829969f327d32eb53eaf7d7a6187549e68624bd1786"
    },
    "kernelspec": {
      "display_name": "Python 3.7.1 64-bit ('deep_learning': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
